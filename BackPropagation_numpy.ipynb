{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8865bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "def Parameter_initialization(Net_dim):\n",
    "    \"\"\"\n",
    "    Initialize the parameters used in both forward and backward propagation.\n",
    "    We only initialize the weight and the activation neurons (treat the input \n",
    "    as 0-th layer's activation neurons) since we need to perform a bias trick,\n",
    "    i.e. transfrom the funtion from 'y = W * X + b' to 'y = W * X'. \n",
    "    \n",
    "    Input:\n",
    "    - Net_dim: A tuple specifying the dimension of our Neural Network\n",
    "    \n",
    "    Outputs:\n",
    "    - W: A list containing weight parameters with bias trick been performed\n",
    "    - a: A list containing initialized activation neurons with bias trick been\n",
    "         performed\n",
    "    \"\"\"    \n",
    "    W = {}\n",
    "    a = {}\n",
    "    for i in range(len(Net_dim)):\n",
    "    ###########################################################################\n",
    "    # In the input layer and hidden layers, we need perform a bias trick by   #\n",
    "    # creating two large enough spaces for W and a then specify a row of zeros#\n",
    "    # as the last row of W (as we initialize b = 0) and a column of ones as   #\n",
    "    # the last column of a.                                                   #\n",
    "    # In the output layer we don't have a bias term since forward propagation #\n",
    "    # stops here.                                                             #\n",
    "    # Note that here we receive the training data one example at a time and   #\n",
    "    # that is why we initialize a with a shape of (1,...).                    #          \n",
    "    ###########################################################################    \n",
    "        if i < len(Net_dim) - 1:\n",
    "            W['W%d' %(i + 1)] = np.random.randn(Net_dim[i] + 1, Net_dim[i + 1])\n",
    "            W['W%d' %(i + 1)][-1, :] = np.zeros((1, Net_dim[i + 1])) \n",
    "            a['a%d' %(i)] = np.zeros((1, Net_dim[i] + 1))\n",
    "            a['a%d' %(i)][:, -1]=np.ones(1)\n",
    "            \n",
    "        else:\n",
    "            a['a%d'%(i)] = np.zeros((1, Net_dim[i]))\n",
    "            \n",
    "    return W,a\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Perform the sigmoid function calculation.\n",
    "    \"\"\"  \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def dsigmoid(z):\n",
    "    \"\"\"\n",
    "    Calculate the derviative of sigmoid function.\n",
    "    \"\"\" \n",
    "    return z * (1 - z)\n",
    "\n",
    "def forward(W, a, function='cross_entropy'):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation and store results used in back propagation.\n",
    "    \n",
    "    Inputs:\n",
    "    - W: A list containing weight parameters with bias trick been performed\n",
    "    - a: A list containing initialized activation neurons with bias trick been\n",
    "         performed\n",
    "    - function: Loss function selected as 'cross_entropy' or 'mean_square'\n",
    "    \n",
    "    Outputs:\n",
    "    - loss: Numerical value of loss function w.r.t one training example \n",
    "    - cache: A list storing the net input neurons and activation neurons \n",
    "    \"\"\" \n",
    "    ###########################################################################\n",
    "    # Note that the output predicted y is the activation function of output   #\n",
    "    # layer.                                                                  #          \n",
    "    ###########################################################################    \n",
    "    z = {}\n",
    "    cache = {}\n",
    "    for l in range(len(Net_dim) - 1):  \n",
    "        z['z%d' %(l + 1)] = a['a%d' %l].dot(W['W%d' %(l + 1)])\n",
    "        \n",
    "        if l < len(Net_dim)-2:\n",
    "            a['a%d' %(l + 1)][:, :-1] = sigmoid(z['z%d' %(l + 1)])\n",
    "            \n",
    "        else:\n",
    "            a['a%d' %(l + 1)] = sigmoid(z['z%d' %(l + 1)])\n",
    "            \n",
    "    y_pred = a['a%d' %(l + 1)]\n",
    "    cache['z'] = z\n",
    "    cache['a'] = a\n",
    "    \n",
    "    if function == 'cross_entropy':\n",
    "        loss = (-(1 - y)*np.log(1 - y_pred) - y * np.log(y_pred)).sum()\n",
    "        \n",
    "    elif function == 'mean_square':\n",
    "        loss = np.square(y_pred - y).sum()\n",
    "        \n",
    "    return loss,cache\n",
    "\n",
    "def backward_and_update(y, W, cache, function='cross_entropy', learning_rate =\n",
    "                        4e-5):\n",
    "    \"\"\"\n",
    "    Perform the back propagation and update weight parameters.\n",
    "    \n",
    "    Inputs:\n",
    "    - y: The actual output in the training set\n",
    "    - W: A list containing weight parameters \n",
    "    - cache: A list storing the net input neurons and activation neurons\n",
    "    - function: Loss function selected as 'cross_entropy' or 'mean_square'\n",
    "    - learning_rate: the step used in updating weight parameters\n",
    "    \n",
    "    Outputs:\n",
    "    - grad: A list containing the partial derivative of loss function w.r.t net\n",
    "            input, activation and weight parameters respectively\n",
    "    - W: A list containing updated weight parameters\n",
    "    \"\"\" \n",
    "    grad = {}\n",
    "    ###########################################################################\n",
    "    # Here we calculate the output layer and previous layers separately. Note # \n",
    "    # that the weight is updated once the corresponding gradient is calculated#\n",
    "    # instead of being updated together after all gradients is obtained.      # \n",
    "    ###########################################################################\n",
    "    idx = len(Net_dim) - 1   \n",
    "    if function == 'cross_entropy':    \n",
    "        grad['a%d' %idx] = (cache['a']['a%d' %idx] - y ) / \\\n",
    "                           (cache['a']['a%d' %idx] * \n",
    "                            (1 - cache['a']['a%d' %idx])) \n",
    "                                 \n",
    "    elif function =='mean_square':\n",
    "        grad['a%d' %idx] = 2.0 * (cache['a']['a%d' %idx] - y)\n",
    "        \n",
    "    grad['z%d' %idx] = dsigmoid(cache['a']['a%d' %idx]) * grad['a%d' %idx]\n",
    "    grad['W%d' %idx] = cache['a']['a%d' %(idx - 1)].T.dot(grad['z%d' %idx])\n",
    "    W['W%d' %idx] -= learning_rate * grad['W%d' %idx]                         \n",
    "\n",
    "    for l in range(2, len(Net_dim)):\n",
    "        idx = len(Net_dim)-l\n",
    "        grad['a%d' %idx] = grad['z%d' %(idx + 1)].dot \\\n",
    "                           (W['W%d' %(idx + 1)].T)\n",
    "        grad['z%d' %idx] = dsigmoid(cache['a']['a%d' %idx][:, :-1]) \\\n",
    "                           * grad['a%d'%idx][:, :-1]\n",
    "        grad['W%d' %idx] = cache['a']['a%d' %(idx - 1)].T.dot(grad['z%d' %idx])\n",
    "        W['W%d' %idx] -= learning_rate * grad['W%d' %idx]\n",
    "        \n",
    "    return grad, W  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d3d17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5312279981887271\n",
      "100 1.4763634529562275\n",
      "200 1.4344460430793509\n",
      "300 1.4055128426540926\n",
      "400 1.3866382951730551\n",
      "500 1.374239621188091\n",
      "600 1.365797353843202\n",
      "700 1.3598060112030785\n",
      "800 1.3553882887309956\n",
      "900 1.3520216494587332\n",
      "1000 1.3493835825480978\n",
      "1100 1.3472674369795723\n",
      "1200 1.3455360538539554\n",
      "1300 1.3440954582926272\n",
      "1400 1.342879419521293\n",
      "1500 1.3418400806316941\n",
      "1600 1.3409420901163314\n",
      "1700 1.340158818767582\n",
      "1800 1.3394698559912723\n",
      "1900 1.3388593129508453\n",
      "2000 1.3383146474972938\n",
      "2100 1.3378258343949818\n",
      "2200 1.337384768897694\n",
      "2300 1.3369848310759804\n",
      "2400 1.3366205628442451\n",
      "2500 1.336287425281471\n",
      "2600 1.335981614010205\n",
      "2700 1.3356999171321042\n",
      "2800 1.3354396047525403\n",
      "2900 1.3351983422281628\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################\n",
    "# Set up the architecture of our neural network and initialize what we need.# \n",
    "# After we read the training data, we perform the forward, backward propaga-#\n",
    "# tion and parameter updating with one example entering into our network at #\n",
    "# a time. Finally, print the data loss per epoch (namely, go through all the#\n",
    "# examples of the training set one time)                                    #\n",
    "#############################################################################\n",
    "\n",
    "Net_dim = [5, 4, 3, 2, 2]\n",
    "W, a = Parameter_initialization(Net_dim)\n",
    "\n",
    "loss_record=[]\n",
    "#a['a0'][:,:-1]=np.random.randn(1,4)\n",
    "#y = np.random.randn(1,1)\n",
    "X = 2 * np.random.randn(50, Net_dim[0])\n",
    "#Y = 5 + 3 * X[:,0][:,np.newaxis] + np.random.randn(50, 1)\n",
    "Y = 5 + np.random.randn(50, Net_dim[-1])\n",
    "#Y=np.sum(X,axis=1)\n",
    "for i in range(3000):\n",
    "    for n in range(50):\n",
    "        loss = None\n",
    "        a['a0'][:, :-1] = X[n].reshape(-1, Net_dim[0])\n",
    "        y = Y[n].reshape(-1, Net_dim[-1])\n",
    "        loss, cache = forward(W, a, function='mean_square')#cross_entropy\n",
    "        loss += loss\n",
    "        loss = loss/50\n",
    "        grad, W = backward_and_update(y, W, cache, function='mean_square', learning_rate = 3e-5)\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss)\n",
    "    loss_record.append(loss)\n",
    "plt.plot(loss_record)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1114b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
